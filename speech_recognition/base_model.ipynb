{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2d2868",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13a6e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "data_path = '.' \n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "SEED = 2018\n",
    "\n",
    "# 리스트를 랜덤하게 셔플하는 함수이다\n",
    "def random_shuffle(lst):\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(lst)\n",
    "    return lst\n",
    "\n",
    "# 텍스트 파일을 저장할 폴더를 생성한다.\n",
    "if not os.path.exists('input'):\n",
    "    os.mkdir('input')\n",
    "\n",
    "# 훈련 데이터 전체를 먼저 trn_all.txt에 저장한다\n",
    "trn_all = []\n",
    "trn_all_file = open('input/trn_all.txt', 'w')\n",
    "# 제공된 훈련 데이터 경로를 모두 읽어온다\n",
    "files = glob(os.path.join(os.getcwd(), 'train/train/audio/*/*.wav'))\n",
    "for f in files:\n",
    "    # 배경 소음은 skip한다\n",
    "    if '_background_noise_' in f:\n",
    "        continue\n",
    "\n",
    "    # 정답값(label)과 화자(speaker)정보를 파일명에서 추출한다\n",
    "    label = f.split('/')[-1].split('\\\\')[1]\n",
    "    speaker = f.split('/')[-1].split('\\\\')[-1].split('_')[0]\n",
    "    if label not in labels:\n",
    "        # 10개의 label외 데이터는 20%의 확률로 unknown으로 분류하여 추가한다\n",
    "        label = 'unknown'\n",
    "        if random.random() < 0.2:\n",
    "            trn_all.append((label, speaker, f))\n",
    "            trn_all_file.write('{},{},{}\\n'.format(label, speaker, f))\n",
    "    else:\n",
    "        trn_all.append((label, speaker, f))\n",
    "        trn_all_file.write('{},{},{}\\n'.format(label, speaker, f))\n",
    "trn_all_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bb2f99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터를 화자 기반 9:1 비율로 분리한다\n",
    "uniq_speakers = list(set([speaker for (label, speaker, path) in trn_all]))\n",
    "random_shuffle(uniq_speakers)\n",
    "cutoff = int(len(uniq_speakers) * 0.9)\n",
    "speaker_val = uniq_speakers[cutoff:]\n",
    "\n",
    "# 교차 검증용 파일을 생성한다\n",
    "trn_file = open('input/trn.txt', 'w')\n",
    "val_file = open('input/val.txt', 'w')\n",
    "for (label, speaker, path) in trn_all:\n",
    "    if speaker not in speaker_val:\n",
    "        trn_file.write('{},{},{}\\n'.format(label, speaker, path))\n",
    "    else:\n",
    "        val_file.write('{},{},{}\\n'.format(label, speaker, path))\n",
    "trn_file.close()\n",
    "val_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818ef74",
   "metadata": {},
   "source": [
    "# 데이터 전처리 루틴 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a5752b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "# 음성 파일의 sample rate은 1초 = 16000으로 지정한다\n",
    "SR = 16000\n",
    "\n",
    "# 경진대회 전용 torch SpeechDataset 클래스를 정의한다\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, mode, label_to_int, wav_list, label_list=None):\n",
    "        self.mode = mode\n",
    "        self.label_to_int = label_to_int\n",
    "        self.wav_list = wav_list\n",
    "        self.label_list = label_list\n",
    "        self.sr = SR\n",
    "        self.n_silence = int(len(wav_list) * 0.1)\n",
    "\n",
    "        # 배경 소음 데이터를 미리 읽어온다\n",
    "        self.background_noises = [librosa.load(x, sr=self.sr)[0] for x in glob(\"./train/train/audio/_background_noise_/*.wav\")]\n",
    "\n",
    "    def get_one_word_wav(self, idx):\n",
    "        # idx 번째 음성 파일을 1초만큼 읽어온다\n",
    "        wav = librosa.load(self.wav_list[idx], sr=self.sr)[0]\n",
    "        if len(wav) < self.sr:\n",
    "            wav = np.pad(wav, (0, self.sr - len(wav)), 'constant')\n",
    "        return wav[:self.sr]\n",
    "\n",
    "    def get_one_noise(self):\n",
    "        # 배경 소음 데이터 중 랜덤하게 1초를 읽어온다\n",
    "        selected_noise = self.background_noises[random.randint(0, len(self.background_noises) - 1)]\n",
    "        start_idx = random.randint(0, len(selected_noise) - 1 - self.sr)\n",
    "        return selected_noise[start_idx:(start_idx + self.sr)]\n",
    "\n",
    "    def get_mix_noises(self, num_noise=1, max_ratio=0.1):\n",
    "        # num_noise 만큼의 배경 소음을 합성한다\n",
    "        result = np.zeros(self.sr)\n",
    "        for _ in range(num_noise):\n",
    "            result += random.random() * max_ratio * self.get_one_noise()\n",
    "        return result / num_noise if num_noise > 0 else result\n",
    "\n",
    "    def get_silent_wav(self, num_noise=1, max_ratio=0.5):\n",
    "        # 배경 소음 데이터를 silence로 가정하고 불러온다\n",
    "        return self.get_mix_noises(num_noise=num_noise, max_ratio=max_ratio)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 교차검증 모드일 경우에는 ‘silence’를 추가한 만큼이 데이터 크기이고, Test 모드일 경우에는 제공된 테스트 데이터가 전부이다\n",
    "        if self.mode == 'test':\n",
    "            return len(self.wav_list)\n",
    "        else:\n",
    "            return len(self.wav_list) + self.n_silence\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx번째 음성 데이터 하나를 반환한다\n",
    "        if idx < len(self.wav_list):\n",
    "            # 전처리는 mel spectrogram으로 지정한다\n",
    "            # (옵션) 여기서 Data Augmentation을 수행할 수 있다.\n",
    "            wav_numpy = preprocess_mel(self.get_one_word_wav(idx))\n",
    "            wav_tensor = torch.from_numpy(wav_numpy).float()\n",
    "            wav_tensor = wav_tensor.unsqueeze(0)\n",
    "\n",
    "            # 음성 스펙트로그램(spec), 파일 경로(id)와 정답값(label)을 반환한다\n",
    "            if self.mode == 'test':\n",
    "                return {'spec': wav_tensor, 'id': self.wav_list[idx]}\n",
    "            else:\n",
    "                label = self.label_to_int.get(self.label_list[idx], len(self.label_to_int))\n",
    "                return {'spec': wav_tensor, 'id': self.wav_list[idx], 'label': label}\n",
    "        else:\n",
    "            # 배경 소음을 반환한다\n",
    "            wav_numpy = preprocess_mel(self.get_silent_wav(\n",
    "                num_noise=random.choice([0, 1, 2, 3]),\n",
    "                max_ratio=random.choice([x / 10. for x in range(20)])))\n",
    "            wav_tensor = torch.from_numpy(wav_numpy).float()\n",
    "            wav_tensor = wav_tensor.unsqueeze(0)\n",
    "            return {'spec': wav_tensor, 'id': 'silence', 'label': len(self.label_to_int) + 1}\n",
    "\n",
    "# mel spectrogram 전처리 함수이다\n",
    "def preprocess_mel(data, n_mels=40):\n",
    "    spectrogram = librosa.feature.melspectrogram(data, sr=SR, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8db16",
   "metadata": {},
   "source": [
    "# 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b2871955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MaxPool2d\n",
    "\n",
    "# ResNet 모델을 구현한다\n",
    "class ResModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResModel, self).__init__()\n",
    "        # 이번 경진대회에 사용되는 label 개수는 12이다\n",
    "        n_labels = 12\n",
    "        n_maps = 128\n",
    "        # 총 9계층 모델을 쌓는다\n",
    "        self.n_layers = n_layers = 9\n",
    "        # 첫 계층에 사용하는 convolutional 모듈을 정의한다\n",
    "        self.conv0 = torch.nn.Conv2d(1, n_maps, (3, 3), padding=(1, 1), bias=False)\n",
    "        # MaxPooling 모듈을 정의한다\n",
    "        self.pool = MaxPool2d(2, return_indices=True)\n",
    "        # 2계층 이후에 사용하는 convolutional 모듈을 정의한다\n",
    "        self.convs = torch.nn.ModuleList([torch.nn.Conv2d(n_maps, n_maps, (3, 3), padding=1, dilation=1, bias=False) for _ in range(n_layers)])\n",
    "        # BatchNormalization 모듈과 conv 모듈을 조합한다\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            self.add_module(\"bn{}\".format(i + 1), torch.nn.BatchNorm2d(n_maps, affine=False))\n",
    "            self.add_module(\"conv{}\".format(i + 1), conv)\n",
    "        # 최종 계층에는 선형 모듈을 추가한다\n",
    "        self.output = torch.nn.Linear(n_maps, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers + 1):\n",
    "            y = F.relu(getattr(self, \"conv{}\".format(i))(x))\n",
    "            if i == 0:\n",
    "                old_x = y\n",
    "            # 이전 layer의 결과값(old_x)와 이번 layer 결과값(y)을 더하는 것이 residual 모듈이다 \n",
    "            if i > 0 and i % 2 == 0:\n",
    "                x = y + old_x\n",
    "                old_x = x\n",
    "            else:\n",
    "                x = y\n",
    "            # BatchNormalization을 통해 파라미터 값을 정규화한다\n",
    "            if i > 0:\n",
    "                x = getattr(self, \"bn{}\".format(i))(x)\n",
    "            # pooling을 사용할지 True/False로 지정한다\n",
    "            pooling = False\n",
    "            if pooling:\n",
    "                x_pool, pool_indices = self.pool(x)\n",
    "                x = self.unpool(x_pool, pool_indices, output_size=x.size())\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = torch.mean(x, 2)\n",
    "        # 최종 선형 계층을 통과한 결과값을 반환한다\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2316b",
   "metadata": {},
   "source": [
    "# pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8e2a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:23<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(37.3886, device='cuda:0') 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/109 [00:00<?, ?it/s]C:\\Users\\master\\AppData\\Local\\Temp/ipykernel_30744/4136072718.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = softmax(speechmodel(spec))\n",
      "100%|█████████████████████████████████████████| 109/109 [00:15<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 2\n",
      "training epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:10<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(68.1770, device='cuda:0') 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 5\n",
      "training epoch  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(84.3887, device='cuda:0') 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 7\n",
      "training epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(89.0379, device='cuda:0') 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 9\n",
      "training epoch  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(91.1805, device='cuda:0') 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 12\n",
      "training epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(92.4897, device='cuda:0') 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 14\n",
      "training epoch  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(93.4061, device='cuda:0') 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 16\n",
      "training epoch  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:14<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(94.1118, device='cuda:0') 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 19\n",
      "training epoch  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(94.5972, device='cuda:0') 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 21\n",
      "training epoch  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(94.8399, device='cuda:0') 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 24\n",
      "training epoch  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(96.6919, device='cuda:0') 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 26\n",
      "training epoch  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.1868, device='cuda:0') 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 28\n",
      "training epoch  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.4359, device='cuda:0') 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:11<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 31\n",
      "training epoch  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:12<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.4455, device='cuda:0') 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 33\n",
      "training epoch  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.4966, device='cuda:0') 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 35\n",
      "training epoch  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:15<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.6339, device='cuda:0') 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:11<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 38\n",
      "training epoch  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:12<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.6977, device='cuda:0') 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 40\n",
      "training epoch  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.7648, device='cuda:0') 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 43\n",
      "training epoch  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:17<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.8255, device='cuda:0') 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 45\n",
      "training epoch  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 979/979 [02:11<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(97.8829, device='cuda:0') 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 109/109 [00:10<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model trainer\n",
    "\"\"\"\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from time import time\n",
    "from torch.nn import Softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 파일 생성 함수\n",
    "def create_directory(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# 시간 체크\n",
    "def get_time(now, start):\n",
    "    time_in_min = int((now - start) / 60)\n",
    "    return time_in_min\n",
    "\n",
    "# 학습을 위한 기본 설정값을 지정한다\n",
    "BATCH_SIZE = 32  # 데이터 묶음에 해당하는 batch_size는 GPU 메모리에 알맞게 지정한다\n",
    "mGPU = False  # multi-GPU를 사용할 경우에는 True로 지정한다\n",
    "epochs = 20  # 모델이 훈련 데이터를 학습하는 횟수를 지정한다\n",
    "mode = 'cv' # 교차 검증 모드(cv) or 테스트 모드(test)\n",
    "model_name = os.path.join(os.getcwd(), 'model/model_resnet.pth' )# 모델 결과물을 저장할 때 모델 이름을 지정한다\n",
    "\n",
    "# ResNet 모델을 활성화한다\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model = ResModel\n",
    "speechmodel = torch.nn.DataParallel(model()) if mGPU else model()\n",
    "speechmodel = speechmodel.cuda()\n",
    "\n",
    "# SpeechDataset을 활성화한다\n",
    "labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "label_to_int = dict(zip(labels, range(len(labels))))\n",
    "int_to_label = dict(zip(range(len(labels)), labels))\n",
    "int_to_label.update({len(labels): 'unknown', len(labels) + 1: 'silence'})\n",
    "\n",
    "# 모드에 따라 학습 및 검증에 사용할 파일을 선택한다\n",
    "trn = 'input/trn.txt' if mode == 'cv' else 'input/trn_all.txt'\n",
    "tst = 'input/val.txt' if mode == 'cv' else 'input/tst.txt'\n",
    "\n",
    "trn = [line.strip() for line in open(trn, 'r').readlines()]\n",
    "wav_list = [line.split(',')[-1] for line in trn]\n",
    "label_list = [line.split(',')[0] for line in trn]\n",
    "# 학습용 SpeechDataset을 불러온다\n",
    "traindataset = SpeechDataset(mode='train', label_to_int=label_to_int, wav_list=wav_list, label_list=label_list)\n",
    "\n",
    "start_time = time()\n",
    "for e in range(epochs):\n",
    "    print(\"training epoch \", e)\n",
    "    # learning_rate를 epoch마다 다르게 지정한다\n",
    "    learning_rate = 0.01 if e < 10 else 0.001\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, speechmodel.parameters()), lr=learning_rate, momentum=0.9, weight_decay=0.00001)\n",
    "    # 모델을 학습하기 위하여 .train() 함수를 실행한다\n",
    "    speechmodel.train()\n",
    "\n",
    "    total_correct = 0\n",
    "    num_labels = 0\n",
    "    trainloader = DataLoader(traindataset, BATCH_SIZE, shuffle=True)\n",
    "    # 학습을 수행한다\n",
    "    for batch_idx, batch_data in enumerate(tqdm(trainloader)):\n",
    "        speechmodel.train()\n",
    "        # batch_size 만큼의 음성 데이터(spec)와 정답값(label)을 받아온다\n",
    "        spec = batch_data['spec']\n",
    "        label = batch_data['label']\n",
    "        spec, label = Variable(spec.cuda()), Variable(label.cuda())\n",
    "        # 현재 모델의 예측값(y_pred)을 계산한다\n",
    "        y_pred = speechmodel(spec)\n",
    "        _, pred_labels = torch.max(y_pred.data, 1)\n",
    "        correct = (pred_labels == label.data).sum()\n",
    "        # 정답과 예측값간의 차이(loss)를 계산한다 \n",
    "        loss = loss_fn(y_pred, label)\n",
    "\n",
    "        total_correct += correct\n",
    "        num_labels += len(label)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # loss를 기반으로 back-propagation을 수행한다\n",
    "        loss.backward()\n",
    "        # 모델 파라미터를 업데이트한다. (실질적 학습)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 훈련 데이터에서의 정확률을 기록한다\n",
    "    print(\"training accuracy:\", 100. * total_correct / num_labels, get_time(time(), start_time))\n",
    "\n",
    "    # 교차 검증 모드의 경우, 검증 데이터에 대한 정확률을 기록한다\n",
    "    if mode == 'cv':\n",
    "        # 현재 학습 중인 모델을 임시로 저장한다\n",
    "        torch.save(speechmodel.state_dict(), '{}_cv'.format(model_name))\n",
    "        \n",
    "        # 검증 데이터를 불러온다\n",
    "        softmax = Softmax()\n",
    "        tst_list = [line.strip() for line in open(tst, 'r').readlines()]\n",
    "        wav_list = [line.split(',')[-1] for line in tst_list]\n",
    "        label_list = [line.split(',')[0] for line in tst_list]\n",
    "        cvdataset = SpeechDataset(mode='test', label_to_int=label_to_int, wav_list=wav_list)\n",
    "        cvloader = DataLoader(cvdataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # 모델을 불러와 .eval() 함수로 검증 준비를 한다\n",
    "        speechmodel = torch.nn.DataParallel(model()) if mGPU else model()\n",
    "        speechmodel.load_state_dict(torch.load('{}_cv'.format(model_name)))\n",
    "        speechmodel = speechmodel.cuda()\n",
    "        speechmodel.eval()\n",
    "\n",
    "        # 검증 데이터를 batch_size만큼씩 받아오며 예측값을 저장한다\n",
    "        fnames, preds = [], []\n",
    "        for batch_idx, batch_data in enumerate(tqdm(cvloader)):\n",
    "            spec = Variable(batch_data['spec'].cuda())\n",
    "            fname = batch_data['id']\n",
    "            y_pred = softmax(speechmodel(spec))\n",
    "            preds.append(y_pred.data.cpu().numpy())\n",
    "            fnames += fname\n",
    "\n",
    "        preds = np.vstack(preds)\n",
    "        preds = [int_to_label[x] for x in np.argmax(preds, 1)]\n",
    "        fnames = [fname.split('/')[-2] for fname in fnames]\n",
    "        num_correct = 0\n",
    "        for true, pred in zip(fnames, preds):\n",
    "            if true == pred:\n",
    "                num_correct += 1\n",
    "\n",
    "        # 검증 데이터의 정확률을 기록한다\n",
    "        print(\"cv accuracy:\", 100. * num_correct / len(preds), get_time(time(), start_time))\n",
    "\n",
    "# 학습이 완료된 모델을 저장한다\n",
    "create_directory(\"model\")\n",
    "torch.save(speechmodel.state_dict(), model_name)\n",
    "\n",
    "# # 테스트 데이터에 대한 예측값을 파일에 저장한다\n",
    "# print(\"doing prediction...\")\n",
    "# softmax = Softmax()\n",
    "\n",
    "# # 테스트 데이터를 불러온다\n",
    "# tst = [line.strip() for line in open(tst, 'r').readlines()]\n",
    "# wav_list = [line.split(',')[-1] for line in tst]\n",
    "# testdataset = SpeechDataset(mode='test', label_to_int=label_to_int, wav_list=wav_list)\n",
    "# testloader = DataLoader(testdataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# # 모델을 불러온다\n",
    "# speechmodel = torch.nn.DataParallel(model()) if mGPU else model()\n",
    "# speechmodel.load_state_dict(torch.load(model_name))\n",
    "# speechmodel = speechmodel.cuda()\n",
    "# speechmodel.eval()\n",
    "    \n",
    "# test_fnames, test_labels = [], []\n",
    "# pred_scores = []\n",
    "\n",
    "# # 테스트 데이터에 대한 예측값을 계산한다\n",
    "# for batch_idx, batch_data in enumerate(tqdm(testloader)):\n",
    "#     spec = Variable(batch_data['spec'].cuda())\n",
    "#     fname = batch_data['id']\n",
    "#     y_pred = softmax(speechmodel(spec))\n",
    "#     pred_scores.append(y_pred.data.cpu().numpy())\n",
    "#     test_fnames += fname\n",
    "\n",
    "# # 가장 높은 확률값을 가진 예측값을 label 형태로 저장한다\n",
    "# final_pred = np.vstack(pred_scores)\n",
    "# final_labels = [int_to_label[x] for x in np.argmax(final_pred, 1)]\n",
    "# test_fnames = [x.split(\"/\")[-1] for x in test_fnames]\n",
    "\n",
    "# # 테스트 파일 명과 예측값을 sub 폴더 아래 저장한다. 캐글에 직접 업로드 할 수 있는 파일 포맷이다.\n",
    "# create_directory(\"sub\")\n",
    "# pd.DataFrame({'fname': test_fnames, 'label': final_labels}).to_csv(\"sub/{}.csv\".format(model_name.split('/')[-1]), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
